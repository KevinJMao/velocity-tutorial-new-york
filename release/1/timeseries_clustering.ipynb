{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries correlation analysis and clustering\n",
    "\n",
    "by Bart De Vylder and Pieter Buteneers from CoScale\n",
    "\n",
    "\n",
    "## Intro\n",
    "\n",
    "In this part we'll illustrate correlation analysis, clustering and dimensionality reduction based on a set of timeseries which are the CPU usages of some pysical nodes and the containers running on them. \n",
    "\n",
    "The analysis shown here was insipred by the sklearn application example for visualizing the stock market structure (http://scikit-learn.org/stable/auto_examples/applications/plot_stock_market.html).\n",
    "\n",
    "We start with importing all the necessary packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import pickle_helper\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from sklearn import cluster, manifold, covariance\n",
    "\n",
    "# Python 3 compatibility\n",
    "from __future__ import print_function   # turns print into a function\n",
    "from __future__ import division         # makes sure 3/2 = 1.5 and not 1 (use 3//2 = 1 instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and inspecting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the cpu usage of a production environment for some physical nodes and containers at 1 minute resolution for a time span of about 8 hours: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, ts_data = pickle_helper.load('data/correlation_series_cpu.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the names of the nodes, \n",
    "- host... are the physical machines\n",
    "- cassandra... are the nodes of a cassandra cluster\n",
    "- elasticsearch... are the nodes of an elasticsearch cluster\n",
    "- dns... are the nodes of a dns service\n",
    "- rumdatareceiver is a component which handles incoming rum (Real User Monitoring) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in nodes:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We inspect the shape of the data: the 27 different timeseries are each in a row of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reorder the nodes and timeseries so the node names are sorted alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = np.argsort(nodes)\n",
    "nodes = nodes[sort_idx]\n",
    "ts_data = ts_data[sort_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do a quick visual check whether the data makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "for node, timeseries in zip(nodes, ts_data):\n",
    "    plt.plot(timeseries, label=node)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.xlabel(\"Time (minutes)\")\n",
    "plt.ylabel(\"CPU usage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to find some structure in this data: we're going to calculate the correlations between any two of these timeseries. The correlation between any two variables is a number between -1 and 1 which measures the degree at which deviations in one variable co-occur with deviations in another in the same direction. The correlation of a variable $V$ with itself is $1$, the correlation of a variable $V$ with its opposite $-V$ is $-1$. \n",
    "\n",
    "\n",
    "First, to understand what's going on, we'll calculate the empirical correlations between the timeseries directly. \n",
    "\n",
    "For that we first convert our dataset such that each timeseries has 0 mean and a standard deviation of 1. The standard deviation of a numpy array can be calculated with  [np.std](http://docs.scipy.org/doc/numpy/reference/generated/numpy.std.html) choosing the correct axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "45b1874f7adf23d48f3b51fc0c3a5e38",
     "grade": false,
     "grade_id": "norm",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# First the data is copied and transposed so it fits the covariance estimator we'll used later on: now each timeseries is in \n",
    "# a column of the table (along axis=0). \n",
    "norm_ts_data = ts_data.copy().T\n",
    "\n",
    "# Then we centralize the data and make sure it has a standard deviation of 1\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "\n",
    "# subtract the mean of each timeseries\n",
    "# norm_ts_data -= ...\n",
    "# divide by the standard deviation\n",
    "# norm_ts_data /= ...\n",
    "\n",
    "\n",
    "# Some tests to see the result makes sense\n",
    "rounding_error_allowed = 1e-10\n",
    "assert norm_ts_data.shape == (501, 27)\n",
    "assert np.all(np.abs(np.mean(norm_ts_data, axis=0)) < rounding_error_allowed)\n",
    "assert np.all(np.abs(np.std(norm_ts_data, axis=0) - 1)  < rounding_error_allowed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the correlation matrix of our set of timeseries where \n",
    "$$ \\text{Corr}_{i,j} = \\text {correlation of } \\text{ts}_i \\text{and } \\text{ts}_j $$\n",
    "\n",
    "Because the data is already normalized, this boils down to calculating the dot-product of the two timeseries:\n",
    "$$ \\text{Corr}_{i,j} = \\frac{1}{N} \\sum_{t} \\text{ts}_i(t) \\text{ts}_j(t) $$\n",
    "\n",
    "with $N$ the length of the timeseries.\n",
    "    \n",
    "which means all correlations can be calculated in one matrix-multiplication using [np.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7a0ac99a007c0f91581f2161f241baa9",
     "grade": false,
     "grade_id": "dot",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "#C_direct = ...\n",
    "\n",
    "assert C_direct.shape == (27, 27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting correlation matrix is a square, symmetrical matrix with values between -1 and 1. The values on the diagonal are 1 because each timeseries is maximally correlated with itself. \n",
    "\n",
    "We can visualize the correlations using matplotlib's [pcolor](https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.pcolor) method, which allows to visualize a matrix using color codes. Let's use a helper function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlations(corr_mat, names):\n",
    "    K = len(names)\n",
    "    assert corr_mat.shape == (K, K)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(corr_mat, cmap=plt.cm.Blues)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_xticks(np.arange(K)+0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(K)+0.5, minor=False)\n",
    "\n",
    "    # We want the diagonal to from topleft to bottomright\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.tick_top()\n",
    "\n",
    "    ax.set_xticklabels(names, minor=False, rotation='vertical')\n",
    "    ax.set_yticklabels(names, minor=False)\n",
    "    ax.set_aspect(1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the correlations we calculated directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlations(C_direct, nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the diagonal pattern clearly emerge. We also see very strong correlation between other timeseries. (Determining a correlation matrix this way using linear operations can also be done directly using the the built in numpy method [np.corrcoef](http://docs.scipy.org/doc/numpy/reference/generated/corrcoef.std.html)).  \n",
    "\n",
    "Now, we'll use a more advanced technique to determine correlations called [graph lasso](http://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphLassoCV.html) which is less likely to overestimate the correlations by assigning a penalty to them and doing cross-validation. We also feed it the normalized data because that's more suiteable to recover the structure of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_lasso_model = covariance.GraphLassoCV()\n",
    "graph_lasso_model.fit(norm_ts_data)\n",
    "C_lasso = graph_lasso_model.covariance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the resulting correlation matrix shows it is more modest in it's estimation of the correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlations(C_lasso, nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, based on the correlation coefficients, we try to cluster the timeseries. For this we use a technique called [affinity propagation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.affinity_propagation.html) which works directly on a matrix of similarity scores between the items to cluster, in our case the correlation matrix. It is an iterative technique which sends messages between the items to classify. One advantage of affinity propagation is that it does not require us to specify the number of clusters beforehand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9331fd50e46fcc67d7b4972f47674553",
     "grade": false,
     "grade_id": "affin",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# cluster_centers, labels = ...\n",
    "\n",
    "print(cluster_centers)\n",
    "print(labels)\n",
    "\n",
    "num_clusters = max(labels) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster_centers are the most prototypical datapoints it found for each cluster. If all went well there are 6 clusters found, the labels define in which cluster each datapoint lies.\n",
    "\n",
    "Now let's take a closer look to the clusters found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7697562b04863bee9296bc148fcdb7c2",
     "grade": false,
     "grade_id": "clusters",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary with the cluster id as key and the list of server names as value \n",
    "clusters = {c : [] for c in range(num_clusters)}\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "    # ... populate the clusters dictionary\n",
    "    \n",
    "for c, v in clusters.items():\n",
    "    print(c, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare this with the real layout of the containers on the physical nodes, we see that the real structure is partially discovered. Most of the hosts serving cassandra and elasticsearch have be put into the same cluster because their load is apparantly very alike.  \n",
    "\n",
    "#### Real node layout\n",
    "- host001\n",
    "    - dns001\n",
    "- host006\n",
    "    - dns004\n",
    "- host007\n",
    "    - cassandracl005\n",
    "    - elasticsearch004\n",
    "- host008\n",
    "    - rumdatareceiver002\n",
    "- host009\n",
    "- host010\n",
    "    - cassandracl006\n",
    "    - elasticsearch005\n",
    "- host011\n",
    "    - cassandracl007\n",
    "    - elasticsearch006\n",
    "- host012\n",
    "    - cassandracl008\n",
    "    - elasticsearch007\n",
    "- host013\n",
    "    - cassandracl002\n",
    "    - elasticsearch013\n",
    "- host014\n",
    "    - cassandracl003\n",
    "    - elasticsearch009\n",
    "- host015\n",
    "    - cassandracl011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction / visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we going to try plotting the clusters by using dimensionality reduction. To start, we have data in a 27-dimensional space: if we want to represent a point in time for our data, we need to specify 27 numbers, because we have 27 timeseries. We have also seen, however, that much of the data is strongly correlated, so we have an opportunity to reduce the dimensions of the data (to compress it) without throwing away too much information. \n",
    "\n",
    "In the current case, as we want to visualize our different timeseries in a 2-dimensional plane, we going to only retain 2 dimensions. \n",
    "\n",
    "A well known technique for dimensionality reduction is called [Principle Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA), which finds a linear transformation of the data to a new coordinate system in such that the new components (dimensions) have no correlation with each other and in which the variance of each successive component is maximized. To reduce the data to 2 dimension, it then suffices to only keep the first two components/dimensions.\n",
    "\n",
    "The technique we'll use here is called [local linear embedding](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html). It is based on PCA but allows different linear transformations in different areas of the input space, thereby avoiding overfitting. Original visualization code : (http://scikit-learn.org/stable/auto_examples/applications/plot_stock_market.html) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a dense eigen_solver to achieve reproducibility (arpack is\n",
    "# initiated with random vectors that we don't control). In addition, we\n",
    "# use a large number of neighbors to capture the large-scale structure.\n",
    "node_position_model = manifold.LocallyLinearEmbedding(\n",
    "    n_components=2, eigen_solver='dense', n_neighbors=6)\n",
    "\n",
    "embedding = node_position_model.fit_transform(norm_ts_data.T).T\n",
    "\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_ts_data.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding now contains the coordinates of the nodes to plot in the plane. The following code also makes use of the clusters found to color the nodes, and uses the correlations to draw lines between the nodes with an intensity related to the correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, facecolor='w', figsize=(10, 8))\n",
    "plt.clf()\n",
    "ax = plt.axes([0., 0., 1., 1.])\n",
    "plt.axis('off')\n",
    "\n",
    "# Display a graph of the partial correlations\n",
    "partial_correlations = graph_lasso_model.precision_.copy()\n",
    "d = 1 / np.sqrt(np.diag(partial_correlations))\n",
    "partial_correlations *= d\n",
    "partial_correlations *= d[:, np.newaxis]\n",
    "non_zero = (np.abs(np.triu(partial_correlations, k=1)) > 0.02)\n",
    "\n",
    "# Plot the nodes using the coordinates of our embedding\n",
    "plt.scatter(embedding[0], embedding[1], s=100 * d ** 2, c=labels,\n",
    "            cmap=plt.cm.spectral)\n",
    "\n",
    "# Plot the edges\n",
    "start_idx, end_idx = np.where(non_zero)\n",
    "#a sequence of (*line0*, *line1*, *line2*), where::\n",
    "#            linen = (x0, y0), (x1, y1), ... (xm, ym)\n",
    "segments = [[embedding[:, start], embedding[:, stop]]\n",
    "            for start, stop in zip(start_idx, end_idx)]\n",
    "values = np.abs(partial_correlations[non_zero])\n",
    "lc = LineCollection(segments,\n",
    "                    zorder=0, cmap=plt.cm.hot_r,\n",
    "                    norm=plt.Normalize(0, .7 * values.max()))\n",
    "lc.set_array(values)\n",
    "lc.set_linewidths(15 * values)\n",
    "ax.add_collection(lc)\n",
    "\n",
    "# Add a label to each node. The challenge here is that we want to\n",
    "# position the labels to avoid overlap with other labels\n",
    "for index, (node, label, (x, y)) in enumerate(\n",
    "        zip(nodes, labels, embedding.T)):\n",
    "\n",
    "    dx = x - embedding[0]\n",
    "    dx[index] = 1\n",
    "    dy = y - embedding[1]\n",
    "    dy[index] = 1\n",
    "    this_dx = dx[np.argmin(np.abs(dy))]\n",
    "    this_dy = dy[np.argmin(np.abs(dx))]\n",
    "    if this_dx > 0:\n",
    "        horizontalalignment = 'left'\n",
    "        x = x + .002\n",
    "    else:\n",
    "        horizontalalignment = 'right'\n",
    "        x = x - .002\n",
    "    if this_dy > 0:\n",
    "        verticalalignment = 'bottom'\n",
    "        y = y + .002\n",
    "    else:\n",
    "        verticalalignment = 'top'\n",
    "        y = y - .002\n",
    "    plt.text(x, y, node, size=10,\n",
    "             horizontalalignment=horizontalalignment,\n",
    "             verticalalignment=verticalalignment,\n",
    "             bbox=dict(facecolor='w',\n",
    "                       edgecolor=plt.cm.spectral(label / float(num_clusters)),\n",
    "                       alpha=.6))\n",
    "\n",
    "plt.xlim(embedding[0].min() - .15 * embedding[0].ptp(),\n",
    "         embedding[0].max() + .10 * embedding[0].ptp(),)\n",
    "plt.ylim(embedding[1].min() - .03 * embedding[1].ptp(),\n",
    "         embedding[1].max() + .03 * embedding[1].ptp())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the clustering information was not use to determine the 2-dimensional positioning of the points, it is clear that both techniques (clustering versus dimensionality reduction) decide on a similar proximity of the nodes.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
